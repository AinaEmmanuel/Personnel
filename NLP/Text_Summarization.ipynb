{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrum0BYcHqAmyeJpvFMk0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plthiyagu/Personnel/blob/master/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJPmuUmzNF0y",
        "colab_type": "text"
      },
      "source": [
        "**Text Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMiTxljhBU4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4 as BeautifulSoup\n",
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY_QuE7zNDth",
        "colab_type": "text"
      },
      "source": [
        "https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RevRW-vMRuN",
        "colab_type": "text"
      },
      "source": [
        "Step 1: Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqrL1bBXBlLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetching the content from the URL\n",
        "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
        "article_read = fetched_data.read()\n",
        "\n",
        "# Parsing the URL content and storing in a variable\n",
        "article_parsed= BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
        "\n",
        "# Returning <p> tags\n",
        "paragraphs= article_parsed.find_all('p')\n",
        "\n",
        "article_content = ''\n",
        "\n",
        "# Looping through the paragraphs and adding them to the variable\n",
        "for p in paragraphs:\n",
        "  article_content += p.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy4DWPO3L8Ye",
        "colab_type": "text"
      },
      "source": [
        "Step 2: Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n3z4dtMFFXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def _create_dictionary_table(text_string) -> dict:\n",
        "\n",
        "  # Removing stop words\n",
        "  stop_words= set(stopwords.words(\"english\"))\n",
        "\n",
        "  words= word_tokenize(text_string)\n",
        "\n",
        "  # Reducing words to their root form\n",
        "\n",
        "  stem = PorterStemmer()\n",
        "\n",
        "  # Creating dictionary for the word frequency table\n",
        "\n",
        "  frequency_table=dict()\n",
        "  for wd in words:\n",
        "    wd= stem.stem(wd)\n",
        "    if wd in stop_words:\n",
        "      continue\n",
        "    if wd in frequency_table:\n",
        "      frequency_table[wd] += 1\n",
        "    else:\n",
        "      frequency_table[wd] =1\n",
        "  return frequency_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v170r0Z2LRz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fb2e568f-969e-4e26-a57a-5177570227b9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiX4IvAALzYq",
        "colab_type": "text"
      },
      "source": [
        "Step 3:  Tokenizing the article into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LwZMVQDKe6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "sentences = sent_tokenize(article_content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZMGgizNMesl",
        "colab_type": "text"
      },
      "source": [
        "Step 4: Finding the weighted frequencies of the sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh8wIpOlK7tW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _calculate_sentence_score(sentences,frequency_table) -> dict:\n",
        "  # Algorithm for scoring a sentence by its words\n",
        "  sentence_weight = dict()\n",
        "\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentence_wordcount =(len(word_tokenize(sentence)))\n",
        "    sentence_wordcount_without_stop_words=0\n",
        "    for word_weight in frequency_table:\n",
        "      if word_weight in sentence.lower():\n",
        "        sentence_wordcount_without_stop_words += 1\n",
        "      if sentence[:7] in sentence_weight:\n",
        "        sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
        "      else:\n",
        "        sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
        "    sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\n",
        "\n",
        "    return sentence_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRbhcG3DWwBh",
        "colab_type": "text"
      },
      "source": [
        "Step 5: Calculating the threshold of the sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1-CeJ4uTabj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _calculate_average_score(sentence_weight) -> int:\n",
        "  # Calculating the average score for the sentences\n",
        "  sum_values= 0\n",
        "  for entry in sentence_weight:\n",
        "    sum_values += sentence_weight[entry]\n",
        "  \n",
        "  # Getting sentence average value from source text\n",
        "  average_score = (sum_values / len(sentence_weight))\n",
        "\n",
        "  return average_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR7PIQ8FaQ67",
        "colab_type": "text"
      },
      "source": [
        "Step 6: Getting the summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBChdT6RKsWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_article_summary(sentences,sentence_weight,threshold):\n",
        "  sentence_counter=0\n",
        "  article_summary = ' '\n",
        "\n",
        "  for sentence in sentences:\n",
        "    if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
        "      article_summary += \" \" + sentence\n",
        "      sentence_counter += 1\n",
        "\n",
        "  return article_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfW_8stVKdDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "85a8bc65-00be-4bbd-f0fb-e306b5bd1f9d"
      },
      "source": [
        "#importing libraries\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import bs4 as BeautifulSoup\n",
        "import urllib.request\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "#fetching the content from the URL\n",
        "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
        "\n",
        "article_read = fetched_data.read()\n",
        "\n",
        "#parsing the URL content and storing in a variable\n",
        "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
        "\n",
        "#returning <p> tags\n",
        "paragraphs = article_parsed.find_all('p')\n",
        "\n",
        "article_content = ''\n",
        "\n",
        "#looping through the paragraphs and adding them to the variable\n",
        "for p in paragraphs:  \n",
        "    article_content += p.text\n",
        "\n",
        "\n",
        "def _create_dictionary_table(text_string) -> dict:\n",
        "   \n",
        "    #removing stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words = word_tokenize(text_string)\n",
        "    \n",
        "    #reducing words to their root form\n",
        "    stem = PorterStemmer()\n",
        "    \n",
        "    #creating dictionary for the word frequency table\n",
        "    frequency_table = dict()\n",
        "    for wd in words:\n",
        "        wd = stem.stem(wd)\n",
        "        if wd in stop_words:\n",
        "            continue\n",
        "        if wd in frequency_table:\n",
        "            frequency_table[wd] += 1\n",
        "        else:\n",
        "            frequency_table[wd] = 1\n",
        "\n",
        "    return frequency_table\n",
        "\n",
        "\n",
        "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
        "\n",
        "    #algorithm for scoring a sentence by its words\n",
        "    sentence_weight = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
        "        sentence_wordcount_without_stop_words = 0\n",
        "        for word_weight in frequency_table:\n",
        "            if word_weight in sentence.lower():\n",
        "                sentence_wordcount_without_stop_words += 1\n",
        "                if sentence[:7] in sentence_weight:\n",
        "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
        "                else:\n",
        "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
        "\n",
        "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words\n",
        "\n",
        "       \n",
        "\n",
        "    return sentence_weight\n",
        "\n",
        "def _calculate_average_score(sentence_weight) -> int:\n",
        "   \n",
        "    #calculating the average score for the sentences\n",
        "    sum_values = 0\n",
        "    for entry in sentence_weight:\n",
        "        sum_values += sentence_weight[entry]\n",
        "\n",
        "    #getting sentence average value from source text\n",
        "    average_score = (sum_values / len(sentence_weight))\n",
        "\n",
        "    return average_score\n",
        "\n",
        "def _get_article_summary(sentences, sentence_weight, threshold):\n",
        "    sentence_counter = 0\n",
        "    article_summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
        "            article_summary += \" \" + sentence\n",
        "            sentence_counter += 1\n",
        "\n",
        "    return article_summary\n",
        "\n",
        "def _run_article_summary(article):\n",
        "    \n",
        "    #creating a dictionary for the word frequency table\n",
        "    frequency_table = _create_dictionary_table(article)\n",
        "\n",
        "    #tokenizing the sentences\n",
        "    sentences = sent_tokenize(article)\n",
        "\n",
        "    #algorithm for scoring a sentence by its words\n",
        "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
        "\n",
        "    #getting the threshold\n",
        "    threshold = _calculate_average_score(sentence_scores)\n",
        "\n",
        "    #producing the summary\n",
        "    article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)\n",
        "\n",
        "    return article_summary\n",
        "\n",
        "    \n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMLvSOL7owaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c393d4d6-3f81-4a64-8b71-145bdd3d5243"
      },
      "source": [
        "summary_results = _run_article_summary(article_content)\n",
        "print(summary_results)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Humans explored space for the first time, taking their first footsteps on the Moon. However, these same wars resulted in the destruction of the imperial system. The victorious Bolsheviks then established the Soviet Union, the world's first communist state. At the beginning of the period, the British Empire was the world's most powerful nation,[15] having acted as the world's policeman for the past century. In total, World War II left some 60 million people dead. At the beginning of the century, strong discrimination based on race and sex was significant in general society. During the century, the social taboo of sexism fell. Communications and information technology, transportation technology, and medical advances had radically altered daily lives. Since the US was in a dominant position, a major part of the process was Americanization. Terrorism, dictatorship, and the spread of nuclear weapons were pressing global issues. Millions were infected with HIV, the virus which causes AIDS. This includes deaths caused by wars, genocide, politicide and mass murders. Later in the 20th century, the development of computers led to the establishment of a theory of computation.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}